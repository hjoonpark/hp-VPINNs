>>>> START
/nobackup/joon/1_Projects/hp-VPINNs/main-torch/Poisson-2D/GaussJacobiQuadRule_V3.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  j = torch.tensor(j).float()
/nobackup/joon/anaconda3/envs/torch-lap/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
N_testfcn_total: 4 4
NE_x: 4 , NE_y: 4
N_testfcn_total: [[5, 5, 5, 5], [5, 5, 5, 5]]
grid_x: tensor([-1.0000, -0.5000,  0.0000,  0.5000,  1.0000])
grid_y: tensor([-1.0000, -0.5000,  0.0000,  0.5000,  1.0000])
x_quad: torch.Size([100, 1])
y_quad: torch.Size([100, 1])
w_quad: torch.Size([100, 2])
F_ext_total: torch.Size([4, 4, 5, 5])
----- Training starts -----
It: 0, Loss: 1.515e+02, Time: 0.74
It: 100, Loss: 1.513e+02, Time: 31.84
It: 200, Loss: 1.516e+02, Time: 33.79
It: 300, Loss: 1.533e+02, Time: 36.44
It: 400, Loss: 1.536e+02, Time: 36.59
It: 500, Loss: 1.538e+02, Time: 40.38
It: 600, Loss: 1.539e+02, Time: 39.60
It: 700, Loss: 1.539e+02, Time: 39.66
It: 800, Loss: 1.540e+02, Time: 31.98
It: 900, Loss: 1.553e+02, Time: 39.58
It: 1000, Loss: 1.725e+02, Time: 39.77
It: 1100, Loss: 2.083e+02, Time: 37.77
It: 1200, Loss: 2.980e+02, Time: 39.93
It: 1300, Loss: 3.793e+02, Time: 39.87
It: 1400, Loss: 5.219e+02, Time: 39.70
It: 1500, Loss: 5.485e+02, Time: 39.94
It: 1600, Loss: 6.461e+02, Time: 34.43
It: 1700, Loss: 6.252e+02, Time: 39.02
It: 1800, Loss: 5.051e+02, Time: 38.88
It: 1900, Loss: 4.551e+02, Time: 39.19
It: 2000, Loss: 4.471e+02, Time: 39.60
It: 2100, Loss: 4.520e+02, Time: 39.42
It: 2200, Loss: 4.598e+02, Time: 39.37
It: 2300, Loss: 4.631e+02, Time: 39.10
It: 2400, Loss: 4.597e+02, Time: 39.61
It: 2500, Loss: 4.568e+02, Time: 38.35
It: 2600, Loss: 4.539e+02, Time: 36.03
It: 2700, Loss: 4.518e+02, Time: 38.97
It: 2800, Loss: 4.506e+02, Time: 39.07
It: 2900, Loss: 4.497e+02, Time: 39.20
It: 3000, Loss: 4.494e+02, Time: 39.84
It: 3100, Loss: 4.492e+02, Time: 39.46
It: 3200, Loss: 4.490e+02, Time: 31.62
It: 3300, Loss: 4.490e+02, Time: 36.15
It: 3400, Loss: 4.492e+02, Time: 31.50
It: 3500, Loss: 4.494e+02, Time: 39.34
It: 3600, Loss: 4.501e+02, Time: 39.37
It: 3700, Loss: 4.510e+02, Time: 39.55
It: 3800, Loss: 4.530e+02, Time: 34.85
It: 3900, Loss: 4.646e+02, Time: 36.10
It: 4000, Loss: 4.937e+02, Time: 38.18
It: 4100, Loss: 5.186e+02, Time: 40.04
It: 4200, Loss: 5.347e+02, Time: 39.73
It: 4300, Loss: 5.451e+02, Time: 39.07
It: 4400, Loss: 5.520e+02, Time: 38.76
It: 4500, Loss: 5.574e+02, Time: 38.97
It: 4600, Loss: 5.610e+02, Time: 38.85
It: 4700, Loss: 5.638e+02, Time: 38.77
It: 4800, Loss: 5.658e+02, Time: 38.66
It: 4900, Loss: 5.678e+02, Time: 35.23
It: 5000, Loss: 5.682e+02, Time: 32.16
It: 5100, Loss: 5.690e+02, Time: 30.36
It: 5200, Loss: 5.691e+02, Time: 37.39
It: 5300, Loss: 5.695e+02, Time: 38.99
It: 5400, Loss: 5.696e+02, Time: 40.06
It: 5500, Loss: 5.701e+02, Time: 38.71
It: 5600, Loss: 5.706e+02, Time: 27.93
It: 5700, Loss: 5.712e+02, Time: 34.65
It: 5800, Loss: 5.723e+02, Time: 39.30
It: 5900, Loss: 5.737e+02, Time: 39.40
It: 6000, Loss: 5.748e+02, Time: 39.40
It: 6100, Loss: 5.762e+02, Time: 39.44
It: 6200, Loss: 5.777e+02, Time: 38.94
It: 6300, Loss: 5.793e+02, Time: 39.23
It: 6400, Loss: 5.808e+02, Time: 38.92
It: 6500, Loss: 5.825e+02, Time: 38.80
It: 6600, Loss: 5.843e+02, Time: 35.53
It: 6700, Loss: 5.855e+02, Time: 38.96
It: 6800, Loss: 5.868e+02, Time: 38.62
It: 6900, Loss: 5.879e+02, Time: 39.48
It: 7000, Loss: 5.889e+02, Time: 38.99
It: 7100, Loss: 5.897e+02, Time: 38.94
It: 7200, Loss: 5.904e+02, Time: 33.50
It: 7300, Loss: 5.909e+02, Time: 38.50
It: 7400, Loss: 5.912e+02, Time: 39.01
It: 7500, Loss: 5.914e+02, Time: 39.01
It: 7600, Loss: 5.914e+02, Time: 30.94
It: 7700, Loss: 5.914e+02, Time: 38.76
It: 7800, Loss: 5.912e+02, Time: 39.41
It: 7900, Loss: 5.909e+02, Time: 39.80
It: 8000, Loss: 5.905e+02, Time: 39.56
It: 8100, Loss: 5.901e+02, Time: 39.48
It: 8200, Loss: 5.896e+02, Time: 31.31
It: 8300, Loss: 5.891e+02, Time: 28.79
It: 8400, Loss: 5.886e+02, Time: 34.14
It: 8500, Loss: 5.881e+02, Time: 36.58
It: 8600, Loss: 5.875e+02, Time: 39.66
It: 8700, Loss: 5.871e+02, Time: 39.60
It: 8800, Loss: 5.865e+02, Time: 39.40
It: 8900, Loss: 5.859e+02, Time: 36.72
It: 9000, Loss: 5.854e+02, Time: 39.18
It: 9100, Loss: 5.849e+02, Time: 38.32
It: 9200, Loss: 5.843e+02, Time: 37.21
It: 9300, Loss: 5.839e+02, Time: 38.75
It: 9400, Loss: 5.833e+02, Time: 39.19
It: 9500, Loss: 5.829e+02, Time: 39.56
It: 9600, Loss: 5.824e+02, Time: 39.59
It: 9700, Loss: 5.821e+02, Time: 39.27
It: 9800, Loss: 5.816e+02, Time: 31.07
It: 9900, Loss: 5.813e+02, Time: 39.50
It: 10000, Loss: 5.809e+02, Time: 35.18
model:
VPINN(
  (net): Sequential(
    (layer_0): Linear(in_features=2, out_features=5, bias=True)
    (activation_0): Tanh()
    (layer_1): Linear(in_features=5, out_features=5, bias=True)
    (activation_1): Tanh()
    (layer_2): Linear(in_features=5, out_features=5, bias=True)
    (activation_2): Tanh()
    (layer_3): Linear(in_features=5, out_features=1, bias=True)
  )
)
u_pred: torch.Size([40401, 1])
----- plotting begins -----
x_train_plot: -1.0 1.0
>>>> DONE
